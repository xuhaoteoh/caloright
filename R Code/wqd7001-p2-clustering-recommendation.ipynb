{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This R environment comes with many helpful analytics packages installed\n# It is defined by the kaggle/rstats Docker image: https://github.com/kaggle/docker-rstats\n# For example, here's a helpful package to load\n\nlibrary(tidyverse) # metapackage of all tidyverse packages\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist.files(path = \"../input\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2021-05-30T12:04:45.033874Z","iopub.execute_input":"2021-05-30T12:04:45.035591Z","iopub.status.idle":"2021-05-30T12:04:46.561674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load nutrition data\nnutritions <- read.csv(\"../input/nutrition-clean/nutrition_cleaned.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:49:36.58611Z","iopub.execute_input":"2021-05-30T12:49:36.587722Z","iopub.status.idle":"2021-05-30T12:49:36.888417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Preparation\nPrior to clustering data, you may want to remove or estimate missing data and rescale variables for comparability.","metadata":{}},{"cell_type":"code","source":"# Prepare Data\nmydata <- nutritions[,4:17]\nmydata <- na.omit(mydata) # listwise deletion of missing\nmydata <- scale(mydata) # standardize variables","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:56:07.881647Z","iopub.execute_input":"2021-05-30T12:56:07.883156Z","iopub.status.idle":"2021-05-30T12:56:07.905084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mydata","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:56:10.579351Z","iopub.execute_input":"2021-05-30T12:56:10.580947Z","iopub.status.idle":"2021-05-30T12:56:10.720761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Partitioning\nK-means clustering is the most popular partitioning method. It requires the analyst to specify the number of clusters to extract. A plot of the within groups sum of squares by number of clusters extracted can help determine the appropriate number of clusters. The analyst looks for a bend in the plot similar to a scree test in factor analysis. See Everitt & Hothorn (pg. 251).","metadata":{}},{"cell_type":"code","source":"# Determine number of clusters\nwss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))\nfor (i in 2:15) wss[i] <- sum(kmeans(mydata,\n   centers=i)$withinss)\nplot(1:15, wss, type=\"b\", xlab=\"Number of Clusters\",\n  ylab=\"Within groups sum of squares\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:56:15.609626Z","iopub.execute_input":"2021-05-30T12:56:15.611008Z","iopub.status.idle":"2021-05-30T12:56:16.006523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Means Cluster Analysis\nfit <- kmeans(mydata, 5) # 5 cluster solution\n# get cluster means\naggregate(mydata,by=list(fit$cluster),FUN=mean)\n# append cluster assignment\nmydata <- data.frame(mydata, fit$cluster)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:56:33.30023Z","iopub.execute_input":"2021-05-30T12:56:33.302413Z","iopub.status.idle":"2021-05-30T12:56:33.384735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A robust version of K-means based on mediods can be invoked by using pam( ) instead of kmeans( ). The function pamk( ) in the fpc package is a wrapper for pam that also prints the suggested number of clusters based on optimum average silhouette width.\n\nHierarchical Agglomerative\nThere are a wide range of hierarchical clustering approaches. I have had good luck with Ward's method described below.","metadata":{}},{"cell_type":"code","source":"# Ward Hierarchical Clustering\nd <- dist(mydata, method = \"euclidean\") # distance matrix\nfit <- hclust(d, method=\"ward\")\nplot(fit) # display dendogram\ngroups <- cutree(fit, k=5) # cut tree into 5 clusters\n# draw dendogram with red borders around the 5 clusters\nrect.hclust(fit, k=5, border=\"red\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:56:51.094306Z","iopub.execute_input":"2021-05-30T12:56:51.095762Z","iopub.status.idle":"2021-05-30T12:56:55.577823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pvclust( ) function in the pvclust package provides p-values for hierarchical clustering based on multiscale bootstrap resampling. Clusters that are highly supported by the data will have large p values. Interpretation details are provided Suzuki. Be aware that pvclust clusters columns, not rows. Transpose your data before using.","metadata":{}},{"cell_type":"code","source":"# Ward Hierarchical Clustering with Bootstrapped p values\nlibrary(pvclust)\nfit <- pvclust(mydata, method.hclust=\"ward\",\n   method.dist=\"euclidean\")\nplot(fit) # dendogram with p values\n# add rectangles around groups highly supported by the data\npvrect(fit, alpha=.95)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:57:10.560092Z","iopub.execute_input":"2021-05-30T12:57:10.562486Z","iopub.status.idle":"2021-05-30T12:58:17.478619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Based\nModel based approaches assume a variety of data models and apply maximum likelihood estimation and Bayes criteria to identify the most likely model and number of clusters. Specifically, the Mclust( ) function in the mclust package selects the optimal model according to BIC for EM initialized by hierarchical clustering for parameterized Gaussian mixture models. (phew!). One chooses the model and number of clusters with the largest BIC. See help(mclustModelNames) to details on the model chosen as best.","metadata":{}},{"cell_type":"code","source":"# Model Based Clustering\nlibrary(mclust)\nfit <- Mclust(mydata)\nplot(fit) # plot results\nsummary(fit) # display the best model","metadata":{"execution":{"iopub.status.busy":"2021-05-30T12:58:17.480654Z","iopub.execute_input":"2021-05-30T12:58:17.481893Z","iopub.status.idle":"2021-05-30T13:03:36.566464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting Cluster Solutions\nIt is always a good idea to look at the cluster results.","metadata":{}},{"cell_type":"code","source":"# K-Means Clustering with 5 clusters\nfit <- kmeans(mydata, 5)\n\n# Cluster Plot against 1st 2 principal components\n\n# vary parameters for most readable graph\nlibrary(cluster)\nclusplot(mydata, fit$cluster, color=TRUE, shade=TRUE,\n   labels=2, lines=0)\n\n# Centroid Plot against 1st 2 discriminant functions\nlibrary(fpc)\nplotcluster(mydata, fit$cluster)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:03:36.568857Z","iopub.execute_input":"2021-05-30T13:03:36.570083Z","iopub.status.idle":"2021-05-30T13:03:38.793699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Validating cluster solutions\nThe function cluster.stats() in the fpc package provides a mechanism for comparing the similarity of two cluster solutions using a variety of validation criteria (Hubert's gamma coefficient, the Dunn index and the corrected rand index)","metadata":{}},{"cell_type":"code","source":"# comparing 2 cluster solutions\nlibrary(fpc)\ncluster.stats(d, fit1$cluster, fit2$cluster)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:03:38.795691Z","iopub.execute_input":"2021-05-30T13:03:38.79689Z","iopub.status.idle":"2021-05-30T13:03:38.818164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"where d is a distance matrix among objects, and fit1$cluster and fit$cluster are integer vectors containing classification results from two different clusterings of the same data.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set plot layout \npar(mfrow=c(3,2),\n     mar=c(2,2,2,1), oma=c(0,0,0,0) # margin: buttom left, top, right\n     )\n\n# training data with first four attributes\ntrainingData <-  mydata\n# 2D plot with two attributes: Sepal.Length Sepal.Width \nplotData <- mydata[,c(1,2)]\n\n# set number of clusters which is a required argument for some clustering algorithms \nnumberOfClusters <- 5\n\n#  -------- K-Means ----------\n\nmodel <- kmeans(trainingData, numberOfClusters)\nplot(plotData, col=model$cluster, main=\"K-Means\")\n# point center of two attributes of plotData\npoints(model$centers[, c(1,3)], col=1:3, pch=8, cex=2)\n\n\n#  -------- Fuzzy C-Means ----------\n\nlibrary(e1071)\n# m - A number greater than 1 giving the degree of fuzzification.\nmodel <- cmeans(trainingData, numberOfClusters, m=2, method=\"cmeans\")\nplot(plotData, col=model$cluster, main=\"Fuzzy C-Means\")\npoints(model$centers[, c(1,3)], col=1:3, pch=8, cex=2)\n\n\n# ------------ Mean Shift  -----------------\n\nlibrary(MeanShift)\nmodel <- msClustering( t(as.matrix(trainingData)), h=0.91 )\nplot(plotData, col=model$labels, main=\"MeanShift\")\n\n\n#  -------- Expectation-Maximization --------\n\nlibrary(mclust)\nmodel <- Mclust(trainingData, numberOfClusters)\nplot(plotData, col=model$classification, main=\"Expectation-Maximization\")\n\n\n#  -------- Density-based -------\n\nlibrary(fpc)\nmodel <- dbscan(trainingData, eps=0.6, MinPts=4)\nplot(plotData, col=4-model$cluster, main=\"Density-based\")\nmtext(\"Noise/outlier observations are coded as 0, and plotted in blue\", cex=0.5)\n\n\n# --------- Hierarchical ------\n\ndistance <- dist(trainingData, method=\"euclidean\") \nhc <- hclust(distance, method=\"average\")\nmodel <- cutree(hc, numberOfClusters)\nplot(plotData, col=model, main=\"Hierarchical\")\n\n\n\n# ---------- Self-Organising Maps -----\n\nlibrary(kohonen) \nsom <- som(as.matrix(scale(trainingData)), somgrid(xdim=5, ydim=30, topo=\"hexagonal\"))\n## use hierarchical clustering to cluster the codebook vectors\nmodel <- cutree(hclust(dist(som$codes[[1]])), numberOfClusters)\nplot(plotData, col=model, main=\"Self-Organising Maps\")\n\n\n# ---------- Spectral ------- \n\nlibrary(kernlab)\nmodel <- specc(as.matrix(trainingData), centers=numberOfClusters)\nplot(plotData, col=model, main=\"Spectral\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}